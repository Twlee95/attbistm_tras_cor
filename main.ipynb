{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\USER\\\\JupyterProjects\\bilstm_attention_ti_cor')\n",
    "from Stock_Dataset import StockDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from Att_BILSTM import attLSTM\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from loss_fn1 import Selective_Regularization\n",
    "from Stock_dataloader_csv_ti import stock_csv_read\n",
    "from Transformer_Encoder import Transformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(attLSTM,transformer, lstm_optimizer, transformer_optimizer,trainset , args): ## Data, loss function, argument\n",
    "\n",
    "    # train dataset을 Dataloader trainset은 주식 개수만큼 append된 list임\n",
    "    tl =[]\n",
    "    for i in range(len(trainset)):\n",
    "        i_trainset = StockDataset(trainset[i][args.split_n])\n",
    "        trainloader = DataLoader(i_trainset, batch_size = args.batch_size,shuffle=False, drop_last=True)\n",
    "        tl.append(trainloader)\n",
    "        \n",
    "    \n",
    "    n_data_bundle = []\n",
    "    for n_data, n_loader in enumerate(tl):\n",
    "        xy = []\n",
    "        i_th_batch = []\n",
    "        for i, (x,y) in enumerate(n_loader):\n",
    "            # print(x.size()) # torch.Size([64, 10, 17])\n",
    "            xy.append(x) \n",
    "            xy.append(y)\n",
    "            i_th_batch.append(xy)\n",
    "        n_data_bundle.append(i_th_batch)\n",
    "            \n",
    "    learning_num = len(tl[0])\n",
    "    attLSTM.train()\n",
    "    transformer.train()     # train단계라는 것을 명시적으로 알려줌, dropout 같은 단계를 자동으로 적용\n",
    "    train_loss = 0.0\n",
    "    for i_th_learning in range(learning_num):\n",
    "        lstm_optimizer.zero_grad()\n",
    "        transformer_optimizer.zero_grad()\n",
    "        for i, data in enumerate(n_data_bundle):\n",
    "            x = data[i_th_learning][0]\n",
    "            y = data[i_th_learning][1]\n",
    "            \n",
    "            x = x.to(args.device)       \n",
    "            y = y.squeeze().float().to(args.device)\n",
    "            \n",
    "            attLSTM.hidden = [hidden.to(args.device) for hidden in attLSTM.init_hidden()]\n",
    "            yhat, attention_weight, attn_applied = attLSTM(x)\n",
    "            \n",
    "            yhat = yhat.unsqueeze(2)\n",
    "            y = y.unsqueeze(1)\n",
    "            if i == 0:\n",
    "                # torch.Size([64])torch.Size([64, 64])\n",
    "                Transformer_input = yhat\n",
    "                True_y = y\n",
    "            else:\n",
    "                Transformer_input = torch.cat((Transformer_input, yhat),dim=2)\n",
    "                True_y = torch.cat((True_y, y),dim=1)\n",
    "                \n",
    "        # print(Transformer_input.size()) torch.Size([64, 10, 50])\n",
    "        output = transformer(Transformer_input.transpose(0,1))\n",
    "\n",
    "        loss = args.loss_fn(output, True_y)\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()    ## parameter 갱신\n",
    "        transformer_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    \n",
    "    del tl\n",
    "    del n_data_bundle\n",
    "    return attLSTM,transformer, train_loss\n",
    "\n",
    "\n",
    "def validation(attLSTM,transformer, valset, args):\n",
    "\n",
    "    # train dataset을 Dataloader\n",
    "    tl =[]\n",
    "    for i in range(len(valset)):\n",
    "        i_valset = StockDataset(valset[i][args.split_n])\n",
    "        valloader = DataLoader(i_valset, batch_size = args.batch_size,shuffle=False, drop_last=True)\n",
    "        tl.append(valloader)\n",
    "        \n",
    "    n_data_bundle = []\n",
    "    for n_data, n_loader in enumerate(tl):\n",
    "        \n",
    "        xy = []\n",
    "        i_th_batch = []\n",
    "        for i, (x,y) in enumerate(n_loader):  \n",
    "            xy.append(x)\n",
    "            xy.append(y)\n",
    "            i_th_batch.append(xy)\n",
    "        n_data_bundle.append(i_th_batch)\n",
    "\n",
    "    attLSTM.eval()\n",
    "    transformer.eval()\n",
    "    val_loss = 0.0\n",
    "    learning_num = len(tl[0]) # 16\n",
    "    with torch.no_grad():\n",
    "        for i_th_learning in range(learning_num):\n",
    "            for i, data in enumerate(n_data_bundle):\n",
    "                \n",
    "                x = data[i_th_learning][0]\n",
    "                y = data[i_th_learning][1]\n",
    "                \n",
    "                x = x.to(args.device)               \n",
    "                y = y.squeeze().float().to(args.device)\n",
    "\n",
    "                attLSTM.hidden = [attLSTM.to(args.device) for hidden in attLSTM.init_hidden()]\n",
    "                yhat, attention_weight, attn_applied = attLSTM(x)\n",
    "                \n",
    "                yhat = yhat.unsqueeze(2)\n",
    "                y = y.unsqueeze(1)\n",
    "                if i == 0:\n",
    "                    # torch.Size([64])torch.Size([64, 64])\n",
    "                    Transformer_input = yhat\n",
    "                    True_y = y\n",
    "                else:\n",
    "                    Transformer_input = torch.cat((Transformer_input, yhat),dim=2)\n",
    "                    True_y = torch.cat((True_y, y),dim=1)\n",
    "                    \n",
    "            output= transformer(Transformer_input.transpose(0,1))\n",
    "            \n",
    "            loss = args.loss_fn(output, True_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    \n",
    "    del tl\n",
    "    del n_data_bundle\n",
    "    \n",
    "    return attLSTM, transformer, val_loss\n",
    "\n",
    "\n",
    "def test(attLSTM, transformer, testset, args):\n",
    "    # train dataset을 Dataloader\n",
    "    tl =[]\n",
    "    for i in range(len(testset)):\n",
    "        i_testset = StockDataset(testset[i][args.split_n])\n",
    "        testloader = DataLoader(i_testset, batch_size = args.batch_size,shuffle=False, drop_last=True)\n",
    "        tl.append(testloader)\n",
    "        \n",
    "    n_data_bundle = []\n",
    "    for n_data, n_loader in enumerate(tl):\n",
    "        xy = []\n",
    "        i_th_batch = []\n",
    "        for i, (x,y) in enumerate(n_loader): \n",
    "\n",
    "            xy.append(x)\n",
    "            xy.append(y)\n",
    "            i_th_batch.append(xy)\n",
    "        n_data_bundle.append(i_th_batch)\n",
    "    \n",
    "    attLSTM.eval()\n",
    "    transformer.eval()\n",
    "    ACC_metric = 0.0\n",
    "    learning_num = len(tl[0])\n",
    "    with torch.no_grad():\n",
    "        for i_th_learning in range(learning_num):\n",
    "            for i,data in enumerate(n_data_bundle):\n",
    "                x = data[i_th_learning][0]\n",
    "                y = data[i_th_learning][1]\n",
    "                # feature transform\n",
    "                y = y.squeeze().float().to(args.device)\n",
    "                x = x.to(args.device)\n",
    "\n",
    "                attLSTM.hidden = [hidden.to(args.device) for hidden in attLSTM.init_hidden()]\n",
    "\n",
    "                yhat, attention_weight, attn_applied = attLSTM(x)\n",
    "                # yhat [64,10]\n",
    "                yhat = yhat.unsqueeze(2)\n",
    "                y = y.unsqueeze(1)\n",
    "                if i == 0:\n",
    "                    # torch.Size([64])torch.Size([64, 64])\n",
    "                    Transformer_input = yhat\n",
    "                    True_y = y\n",
    "                else:\n",
    "                    Transformer_input = torch.cat((Transformer_input, yhat),dim=2)\n",
    "                    True_y = torch.cat((True_y, y),dim=1)\n",
    "\n",
    "            output= transformer(Transformer_input.transpose(0,1))\n",
    "            output_ = torch.where(output >= 0.5, 1.0, 0.0)\n",
    "            output_.requires_grad = True\n",
    "            \n",
    "            perc_y_pred = output_.detach().cpu().numpy()\n",
    "            perc_y_true = True_y.detach().cpu().numpy()\n",
    "            \n",
    "            perc_y_predarr = np.array(perc_y_pred)\n",
    "            perc_y_truearr = np.array(perc_y_true)\n",
    "            \n",
    "            acclist_0 = []               \n",
    "            acclist_1 = []\n",
    "            acclist_2 = []\n",
    "            acclist_3 = []\n",
    "            acclist_4 = []\n",
    "            acclist_5 = []\n",
    "            acclist_6 = []\n",
    "            acclist_7 = []\n",
    "            acclist_8 = []\n",
    "            acclist_9 = []\n",
    "            acclist_10= []\n",
    "            acclist_11= []\n",
    "            acclist_12= []\n",
    "            acclist_13= []\n",
    "            acclist_14= []\n",
    "            acclist_15= []\n",
    "            acclist_16= []\n",
    "            acclist_17= []\n",
    "            acclist_18= []\n",
    "            acclist_19= []\n",
    "            acclist_20= []\n",
    "            acclist_21= []\n",
    "            acclist_22= []\n",
    "            acclist_23= []\n",
    "            acclist_24= []\n",
    "            acclist_25= []\n",
    "            acclist_26= []\n",
    "            acclist_27= []\n",
    "            acclist_28= []\n",
    "            acclist_29= []\n",
    "            acclist_30= []\n",
    "            acclist_31= []\n",
    "            acclist_32= []\n",
    "            acclist_33= []\n",
    "            acclist_34= []\n",
    "            acclist_35= []\n",
    "            acclist_36= []\n",
    "            acclist_37= []\n",
    "            acclist_38= []\n",
    "            acclist_39= []\n",
    "            acclist_40= []\n",
    "            acclist_41= []\n",
    "            acclist_42= []\n",
    "            acclist_43= []\n",
    "            acclist_44= []\n",
    "            acclist_45= []\n",
    "            acclist_46= []\n",
    "            acclist_47= []\n",
    "            acclist_48= []\n",
    "            acclist_49= []\n",
    "            \n",
    "            for i in range(50):\n",
    "                if perc_y_predarr[:,i] != 0:\n",
    "                    globals()['acc_'+str(i)] = accuracy_score(perc_y_predarr[:,i], perc_y_truearr[:,i])\n",
    "                else:\n",
    "                    globals()['acc_'+str(i)] = 0.0\n",
    "                    \n",
    "            acclist_0.append(acc_0)                   \n",
    "            acclist_1.append(acc_1)\n",
    "            acclist_2.append(acc_2)\n",
    "            acclist_3.append(acc_3)\n",
    "            acclist_4.append(acc_4)\n",
    "            acclist_5.append(acc_5)\n",
    "            acclist_6.append(acc_6)\n",
    "            acclist_7.append(acc_7)\n",
    "            acclist_8.append(acc_8)\n",
    "            acclist_9.append(acc_9)\n",
    "            acclist_10.append(acc_10)\n",
    "            acclist_11.append(acc_11)\n",
    "            acclist_12.append(acc_12)\n",
    "            acclist_13.append(acc_13)\n",
    "            acclist_14.append(acc_14)\n",
    "            acclist_15.append(acc_15)\n",
    "            acclist_16.append(acc_16)\n",
    "            acclist_17.append(acc_17)\n",
    "            acclist_18.append(acc_18)\n",
    "            acclist_19.append(acc_19)\n",
    "            acclist_20.append(acc_20)\n",
    "            acclist_21.append(acc_21)\n",
    "            acclist_22.append(acc_22)\n",
    "            acclist_23.append(acc_23)\n",
    "            acclist_24.append(acc_24)\n",
    "            acclist_25.append(acc_25)\n",
    "            acclist_26.append(acc_26)\n",
    "            acclist_27.append(acc_27)\n",
    "            acclist_28.append(acc_28)\n",
    "            acclist_29.append(acc_29)\n",
    "            acclist_30.append(acc_30)\n",
    "            acclist_31.append(acc_31)\n",
    "            acclist_32.append(acc_32)\n",
    "            acclist_33.append(acc_33)\n",
    "            acclist_34.append(acc_34)\n",
    "            acclist_35.append(acc_35)\n",
    "            acclist_36.append(acc_36)\n",
    "            acclist_37.append(acc_37)\n",
    "            acclist_38.append(acc_38)\n",
    "            acclist_39.append(acc_39)\n",
    "            acclist_40.append(acc_40)\n",
    "            acclist_41.append(acc_41)\n",
    "            acclist_42.append(acc_42)\n",
    "            acclist_43.append(acc_43)\n",
    "            acclist_44.append(acc_44)\n",
    "            acclist_45.append(acc_45)\n",
    "            acclist_46.append(acc_46)\n",
    "            acclist_47.append(acc_47)\n",
    "            acclist_48.append(acc_48)\n",
    "            acclist_49.append(acc_49)\n",
    "\n",
    "            ## acc list가 데이터 개수만큼 데이터 순서대로 01234... 생김\n",
    "                    \n",
    "            #ACC_metric += acc\n",
    "            #ACC_metric += ACC(perc_y_pred, perc_y_true) # numpy.float64' object is not callable\n",
    "    acc_metric_list = []\n",
    "    for ACC_metric in globals():\n",
    "        if \"ACC_metric\" in ACC_metric:\n",
    "            ACC_metric = ACC_metric / len(testloader)\n",
    "            acc_metric_list.append(ACC_metric)\n",
    "    return acc_metric_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.save_file_path = \"C:\\\\Users\\\\USER\\\\JupyterProjects\\\\bilstm_attention_ti_cor\\\\results\"\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 64\n",
    "\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "args.loss_fn = nn.BCELoss()  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0005\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 100\n",
    "\n",
    "args.ent_split_n = 10\n",
    "# ============= model ================== #\n",
    "args.attLSTM = attLSTM\n",
    "args.transformer = Transformer\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "\n",
    "args.input_dim = 64\n",
    "args.hid_dim = 64\n",
    "args.output_dim = 1\n",
    "\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.attLSTM_x_frames = 1\n",
    "\n",
    "\n",
    "# ====== transformer hyperparameter ======= #\n",
    "args.trans_feature_size = 5\n",
    "args.trans_num_laysers = 1\n",
    "args.trans_nhead = 1\n",
    "args.market_beta = 0.1\n",
    "\n",
    "# trans_feature_size / trans_nhead => int 필수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss(train/val) 0.69550/0.69935. Took 0.70 sec\n",
      "Epoch 1, Loss(train/val) 0.67839/0.69800. Took 0.55 sec\n",
      "Epoch 2, Loss(train/val) 0.64948/0.71707. Took 0.46 sec\n",
      "Epoch 3, Loss(train/val) 0.58003/0.75956. Took 0.47 sec\n",
      "Epoch 4, Loss(train/val) 0.52053/0.80875. Took 0.46 sec\n",
      "Epoch 5, Loss(train/val) 0.47464/0.86004. Took 0.46 sec\n",
      "Epoch 6, Loss(train/val) 0.43007/0.92672. Took 0.44 sec\n",
      "Epoch 7, Loss(train/val) 0.39932/0.98364. Took 0.45 sec\n",
      "Epoch 8, Loss(train/val) 0.36869/1.03576. Took 0.45 sec\n",
      "Epoch 9, Loss(train/val) 0.34147/1.11022. Took 0.47 sec\n",
      "Epoch 10, Loss(train/val) 0.31730/1.13921. Took 0.49 sec\n",
      "Epoch 11, Loss(train/val) 0.29802/1.18482. Took 0.45 sec\n",
      "Epoch 12, Loss(train/val) 0.27935/1.23398. Took 0.44 sec\n",
      "Epoch 13, Loss(train/val) 0.26747/1.26959. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.25679/1.31356. Took 0.44 sec\n",
      "Epoch 15, Loss(train/val) 0.24750/1.38056. Took 0.50 sec\n",
      "Epoch 16, Loss(train/val) 0.24311/1.39976. Took 0.44 sec\n",
      "Epoch 17, Loss(train/val) 0.23499/1.46313. Took 0.45 sec\n",
      "Epoch 18, Loss(train/val) 0.23358/1.45499. Took 0.46 sec\n",
      "Epoch 19, Loss(train/val) 0.22954/1.50965. Took 0.46 sec\n",
      "Epoch 20, Loss(train/val) 0.22553/1.55895. Took 0.45 sec\n",
      "Epoch 21, Loss(train/val) 0.22489/1.56367. Took 0.44 sec\n",
      "Epoch 22, Loss(train/val) 0.21684/1.61389. Took 0.45 sec\n",
      "Epoch 23, Loss(train/val) 0.21597/1.65675. Took 0.44 sec\n",
      "Epoch 24, Loss(train/val) 0.21488/1.63615. Took 0.43 sec\n",
      "Epoch 25, Loss(train/val) 0.21479/1.64980. Took 0.45 sec\n",
      "Epoch 26, Loss(train/val) 0.21220/1.68861. Took 0.45 sec\n",
      "Epoch 27, Loss(train/val) 0.21051/1.69962. Took 0.44 sec\n",
      "Epoch 28, Loss(train/val) 0.20879/1.71167. Took 0.44 sec\n",
      "Epoch 29, Loss(train/val) 0.20710/1.72769. Took 0.47 sec\n",
      "Epoch 30, Loss(train/val) 0.20733/1.75360. Took 0.45 sec\n",
      "Epoch 31, Loss(train/val) 0.20755/1.77571. Took 0.46 sec\n",
      "Epoch 32, Loss(train/val) 0.20604/1.77143. Took 0.45 sec\n",
      "Epoch 33, Loss(train/val) 0.20454/1.79152. Took 0.44 sec\n",
      "Epoch 34, Loss(train/val) 0.20354/1.78318. Took 0.45 sec\n",
      "Epoch 35, Loss(train/val) 0.20324/1.83820. Took 0.45 sec\n",
      "Epoch 36, Loss(train/val) 0.20185/1.83716. Took 0.45 sec\n",
      "Epoch 37, Loss(train/val) 0.20007/1.84335. Took 0.44 sec\n",
      "Epoch 38, Loss(train/val) 0.20076/1.88613. Took 0.45 sec\n",
      "Epoch 39, Loss(train/val) 0.20082/1.85206. Took 0.44 sec\n",
      "Epoch 40, Loss(train/val) 0.20018/1.86397. Took 0.45 sec\n",
      "Epoch 41, Loss(train/val) 0.20030/1.85771. Took 0.44 sec\n",
      "Epoch 42, Loss(train/val) 0.19814/1.86906. Took 0.44 sec\n",
      "Epoch 43, Loss(train/val) 0.19777/1.87547. Took 0.46 sec\n",
      "Epoch 44, Loss(train/val) 0.19921/1.86176. Took 0.44 sec\n",
      "Epoch 45, Loss(train/val) 0.19807/1.88613. Took 0.44 sec\n",
      "Epoch 46, Loss(train/val) 0.19577/1.88429. Took 0.45 sec\n",
      "Epoch 47, Loss(train/val) 0.19727/1.90881. Took 0.45 sec\n",
      "Epoch 48, Loss(train/val) 0.19668/1.90680. Took 0.46 sec\n",
      "Epoch 49, Loss(train/val) 0.19599/1.94278. Took 0.45 sec\n",
      "Epoch 50, Loss(train/val) 0.19580/1.94871. Took 0.44 sec\n",
      "Epoch 51, Loss(train/val) 0.19491/1.94762. Took 0.45 sec\n",
      "Epoch 52, Loss(train/val) 0.19431/1.94336. Took 0.44 sec\n",
      "Epoch 53, Loss(train/val) 0.19390/1.94051. Took 0.44 sec\n",
      "Epoch 54, Loss(train/val) 0.19339/1.96445. Took 0.45 sec\n",
      "Epoch 55, Loss(train/val) 0.19363/1.94057. Took 0.46 sec\n",
      "Epoch 56, Loss(train/val) 0.19343/1.96925. Took 0.45 sec\n",
      "Epoch 57, Loss(train/val) 0.19258/1.94846. Took 0.45 sec\n",
      "Epoch 58, Loss(train/val) 0.19188/1.96472. Took 0.45 sec\n",
      "Epoch 59, Loss(train/val) 0.19111/1.95132. Took 0.44 sec\n",
      "Epoch 60, Loss(train/val) 0.19093/2.01007. Took 0.44 sec\n",
      "Epoch 61, Loss(train/val) 0.18975/1.99366. Took 0.44 sec\n",
      "Epoch 62, Loss(train/val) 0.18922/1.99961. Took 0.44 sec\n",
      "Epoch 63, Loss(train/val) 0.18879/2.01839. Took 0.45 sec\n",
      "Epoch 64, Loss(train/val) 0.18800/2.00133. Took 0.44 sec\n",
      "Epoch 65, Loss(train/val) 0.18782/2.01077. Took 0.44 sec\n",
      "Epoch 66, Loss(train/val) 0.18609/2.02102. Took 0.45 sec\n",
      "Epoch 67, Loss(train/val) 0.18491/2.04091. Took 0.45 sec\n",
      "Epoch 68, Loss(train/val) 0.18376/2.04089. Took 0.45 sec\n",
      "Epoch 69, Loss(train/val) 0.18262/2.05261. Took 0.44 sec\n",
      "Epoch 70, Loss(train/val) 0.18219/2.04803. Took 0.46 sec\n",
      "Epoch 71, Loss(train/val) 0.18058/2.06292. Took 0.50 sec\n",
      "Epoch 72, Loss(train/val) 0.17875/2.09000. Took 0.45 sec\n",
      "Epoch 73, Loss(train/val) 0.17662/2.10948. Took 0.46 sec\n",
      "Epoch 74, Loss(train/val) 0.17601/2.08679. Took 0.49 sec\n",
      "Epoch 75, Loss(train/val) 0.17367/2.08915. Took 0.47 sec\n",
      "Epoch 76, Loss(train/val) 0.17160/2.12883. Took 0.50 sec\n",
      "Epoch 77, Loss(train/val) 0.16974/2.12655. Took 0.51 sec\n",
      "Epoch 78, Loss(train/val) 0.16686/2.13558. Took 0.47 sec\n",
      "Epoch 79, Loss(train/val) 0.16393/2.15375. Took 0.46 sec\n",
      "Epoch 80, Loss(train/val) 0.16196/2.16072. Took 0.45 sec\n",
      "Epoch 81, Loss(train/val) 0.15946/2.21814. Took 0.45 sec\n",
      "Epoch 82, Loss(train/val) 0.15786/2.22287. Took 0.45 sec\n",
      "Epoch 83, Loss(train/val) 0.15390/2.21432. Took 0.46 sec\n",
      "Epoch 84, Loss(train/val) 0.15197/2.27696. Took 0.45 sec\n",
      "Epoch 85, Loss(train/val) 0.14862/2.26436. Took 0.45 sec\n",
      "Epoch 86, Loss(train/val) 0.14611/2.27581. Took 0.44 sec\n",
      "Epoch 87, Loss(train/val) 0.14367/2.35208. Took 0.45 sec\n",
      "Epoch 88, Loss(train/val) 0.14042/2.30576. Took 0.44 sec\n",
      "Epoch 89, Loss(train/val) 0.13834/2.89845. Took 0.44 sec\n",
      "Epoch 90, Loss(train/val) 0.13485/2.88363. Took 0.44 sec\n",
      "Epoch 91, Loss(train/val) 0.13205/2.93560. Took 0.44 sec\n",
      "Epoch 92, Loss(train/val) 0.12840/3.24897. Took 0.44 sec\n",
      "Epoch 93, Loss(train/val) 0.12725/3.24146. Took 0.47 sec\n",
      "Epoch 94, Loss(train/val) 0.12445/3.26727. Took 0.45 sec\n",
      "Epoch 95, Loss(train/val) 0.11971/3.32154. Took 0.44 sec\n",
      "Epoch 96, Loss(train/val) 0.11831/3.31193. Took 0.45 sec\n",
      "Epoch 97, Loss(train/val) 0.11553/3.40560. Took 0.45 sec\n",
      "Epoch 98, Loss(train/val) 0.11407/3.39882. Took 0.46 sec\n",
      "Epoch 99, Loss(train/val) 0.11161/4.04980. Took 0.45 sec\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36032/2013602959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_file_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msite_val_losses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'_transformer'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mACC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_50_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mACC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ACC: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mACC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36032/1492949110.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(attLSTM, transformer, testset, args)\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0macc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;34m\"acc_\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                             \u001b[0macclists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m## acclist_0: 0번째 데이터의 acclist, acclist_1: : 1번째 데이터의 acclist, acclist_2: : 2번째 데이터의 acclist....., acclist_n: : n번째 데이터의 acclist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m                     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;31m## acc list가 데이터 개수만큼 데이터 순서대로 01234... 생김\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "## 실행 파일\n",
    "file_tf=os.path.isdir(r\"C:\\Users\\USER\\JupyterProjects\\bilstm_attention_ti_cor\\results\\0_split\")\n",
    "if file_tf == True:\n",
    "    sys.exit(\"file is already exist\")\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'ATTBILSTM_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\"])\n",
    "    \n",
    "    # stock = data.split('.')[0]\n",
    "\n",
    "    est = time.time()\n",
    "    # setattr(args, 'symbol', stock)\n",
    "    # args.new_file_path = args.save_file_path + '\\\\' + \"ATTBILSTM_\" + args.symbol\n",
    "    # os.makedirs(args.new_file_path)\n",
    "        \n",
    "    csv_read = stock_csv_read(args.x_frames,args.y_frames)\n",
    "    stock_50_data, data_list = csv_read.cv_split() ## 분할된 50개의 주식, data_list를 한번에 뱉어줌\n",
    "    \n",
    "    ACC_cv = []\n",
    "    for split_n, i_split in enumerate(range(args.ent_split_n)):\n",
    "        args.split_n = split_n\n",
    "        \n",
    "        args.split_file_path = args.save_file_path + '\\\\' + str(args.split_n) + \"_split\"\n",
    "        os.makedirs(args.split_file_path)\n",
    "        \n",
    "        attLSTM = args.attLSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,args.dropout, args.use_bn, args.attention_head, args.attn_size,activation=\"ReLU\")\n",
    "        transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers, args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    \n",
    "        attLSTM.to(args.device)\n",
    "        transformer.to(args.device)\n",
    "        \n",
    "        if args.optim == 'SGD':\n",
    "            lstm_optimizer = optim.SGD(attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "            transformer_optimizer = optim.SGD(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "\n",
    "        elif args.optim == 'RMSprop':\n",
    "            lstm_optimizer = optim.RMSprop(attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "            transformer_optimizer = optim.RMSprop(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        elif args.optim == 'Adam':\n",
    "            lstm_optimizer = optim.Adam(attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "            transformer_optimizer = optim.Adam(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        else:\n",
    "            raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "        # ===== List for epoch-wise data ====== #\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        # ===================================== #\n",
    "        for epoch in range(args.epoch):\n",
    "            ts = time.time()\n",
    "            attLSTM, transformer, train_loss = train(attLSTM, transformer, lstm_optimizer, transformer_optimizer, stock_50_data[0], args)\n",
    "            attLSTM, transformer, val_loss = validation(attLSTM, transformer, stock_50_data[1], args)\n",
    "\n",
    "            te = time.time()\n",
    "\n",
    "            ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "            if len(val_losses) == 0:\n",
    "                torch.save(attLSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_attLSTM' +'.pt')\n",
    "                torch.save(transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_transformer' +'.pt')\n",
    "            elif min(val_losses) > val_loss:\n",
    "                torch.save(attLSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_attLSTM' +'.pt')\n",
    "                torch.save(transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_transformer' +'.pt')\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                    .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "        ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "        site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "        attLSTM = args.attLSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                                            args.dropout, args.use_bn, args.attention_head, args.attn_size, activation=\"ReLU\")\n",
    "        transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "\n",
    "        attLSTM.to(args.device)\n",
    "        transformer.to(args.device)\n",
    "        \n",
    "        attLSTM.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) +'_attLSTM'+ '.pt'))\n",
    "        transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) +'_transformer' + '.pt'))\n",
    "\n",
    "        ACC = test(attLSTM, transformer, stock_50_data[2], args)\n",
    "        print(ACC)\n",
    "        print('ACC: {}'.format(ACC))\n",
    "\n",
    "        with open(args.split_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "            print('ACC: {}'.format(ACC), file=fd)\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        result['train_losses'] = train_losses\n",
    "        result['val_losses'] = val_losses\n",
    "        result['ACC'] = ACC\n",
    "    \n",
    "    # =============================================================================================================================================== #\n",
    "        eet = time.time()\n",
    "        entire_exp_time = eet - est\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.plot(result['train_losses'])\n",
    "        plt.plot(result['val_losses'])\n",
    "        plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "        plt.xlabel('epoch', fontsize=15)\n",
    "        plt.ylabel('loss', fontsize=15)\n",
    "        plt.grid()\n",
    "        plt.savefig(args.split_file_path + '\\\\' + 'fig' + '.png')\n",
    "        plt.close(fig)\n",
    "        ACC_cv.append(result['ACC'])\n",
    "        \n",
    "    ACC_cv_ar = np.array(ACC_cv)\n",
    "    acc_avg = np.mean(ACC_cv_ar)\n",
    "    acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "    wr.writerow([\"BILSTM_ATTENTION\", args.symbol, entire_exp_time, acc_avg, acc_std])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "ff4208fb53146a003fd3ddce22903cec0cd243c3b14a4aec7cc1b5c53908f821"
  },
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
